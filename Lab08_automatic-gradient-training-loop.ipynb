{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaluznys/uczenie_maszynowe_UW/blob/main/Lab08_automatic-gradient-training-loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Working with Other Loss Functions**\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "During the class today, we reconstructed an **ellipse**. The ellipse was defined with two **foci** and $C$ (the sum of distances of the ellipse points from the foci).\n",
        "\n",
        "To reconstruct the ellipse through optimization, we began with **$N$ points** scattered randomly in the 2D plane. Our goal was to adjust their positions so that they satisfy the elliptical constraint as closely as possible. We achieved this by minimizing the **error-related loss**, $L^{(2)}_{\\text{ellipse}}$ which was defined in today's class with $\\ell_2$ norm as:\n",
        "\n",
        "$$\n",
        "L^{(2)}_{\\text{ellipse}} = \\frac{1}{N} \\sum_{i=1}^{N} \\epsilon_i^2\n",
        "$$\n",
        "\n",
        "where\n",
        "$$\n",
        " \\epsilon_i = d_{i1} + d_{i2} - C\n",
        "$$\n",
        "where $N$ is the number of points, and $d_{i1}, d_{i2}$ are their distances to the two foci.\n",
        "\n",
        "In the homework assignment you will experiment with 3 other loss definitions based on some other possible norms.\n",
        "\n",
        "\n",
        "\n",
        "1. $\\ell_0$ norm resulting in\n",
        "  $$\n",
        "  L^{(0)}_{\\text{ellipse}} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}(\\epsilon_i \\neq 0)\n",
        "  $$\n",
        "  - If you actually succeed to code this loss function, the question for you to answer in relation to $L^{(0)}_{\\text{ellipse}}$ is why the training is not progressing with passing epochs.\n",
        "  - If you actually fail to code this loss function, the question for you to answer is to explain the failure and reason out theoretically, why the training would not be progressing with passing epochs, anyway.\n",
        "\n",
        "1. $\\ell_1$ norm resulting in\n",
        "  $$\n",
        "  L^{(1)}_{\\text{ellipse}} = \\frac{1}{N} \\sum_{i=1}^{N} |\\epsilon_i|\n",
        "  $$\n",
        "  The question for you to answer in relation to $L^{(1)}_{\\text{ellipse}}$ is why the training loss doesn't converge, even after the ellipse has been fully drawn.\n",
        "\n",
        "1. $\\ell_\\infty$ norm resulting in\n",
        "  $$\n",
        "  L^{(\\infty)}_{\\text{ellipse}} =  \\max_{i} |\\epsilon_i|\n",
        "  $$\n",
        "  The question for you to answer in relation to $L^{(\\infty)}_{\\text{ellipse}}$ is why the training takes so long and it doesn't converge in the end, either.\n",
        "\n",
        "## **Points to Note**\n",
        "\n",
        "1. Draw both the shape that the points draw as they move, and the loss value after each epoch, just as we did in class today.\n",
        "\n",
        "2. Note, that the purpose of this excercise is not that you reconstruct a perfect ellipse, but rather that you give it a try, and even if you fail you should document and explain the failure, and answer a question related to a given loss definition.\n",
        "\n",
        "3. You can also play around with the learning rate to try to improve convergence.\n",
        "\n",
        "## **Task & Deliverables**\n",
        "  \n",
        "   - Document your experiments (python code and charts) and **write down your conclusions** into the Colab notebook.\n",
        "   - It is not strictly required, but **if you make a movie showing the optimization progress it will be considered a strong point of your solution**\n",
        "     - You can make a movie programmatically as we did in clustering class (our second class) with EM clustering,\n",
        "     - or, you can save to disk the image files with epoch charts and use an external tool to bind them into a movie. Provide links to movie files in the README.\n",
        "   - Place the Colab notebook  with the solution in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook (and any movies you created) and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
        "\n",
        "## Sample code\n",
        "\n",
        "   You can use the sample code provided below:\n",
        "\n"
      ],
      "metadata": {
        "id": "aiWSlVV6wCYP"
      },
      "id": "aiWSlVV6wCYP"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Fix the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define ellipse parameters\n",
        "focus1 = torch.tensor([-2.0, 0.0])  # First focus\n",
        "focus2 = torch.tensor([2.0, 0.0])   # Second focus\n",
        "constant_sum = 6.0  # The sum of distances to the two foci\n",
        "\n",
        "# Initialize random 2D points from uniform distribution\n",
        "num_points = 100\n",
        "points = torch.rand((num_points, 2)) * 10 - 5  # Uniformly distributed in [-5, 5]\n",
        "points.requires_grad = True\n",
        "\n",
        "# Reset trajectories\n",
        "trajectories = [[] for _ in range(num_points)]\n",
        "loss_history = []\n",
        "\n",
        "\n",
        "import time\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam([points], lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute distances to both foci\n",
        "    dist1 = torch.norm(points - focus1, dim=1)\n",
        "    dist2 = torch.norm(points - focus2, dim=1)\n",
        "\n",
        "    torch.set_printoptions(profile=\"full\")\n",
        "    # Compute loss - it is implemented as MSE loss related to l_2 norm\n",
        "    # TODO: your job is to replace the line below\n",
        "    # with l_0, l_1 and l_infty - related loss\n",
        "    loss = torch.mean((dist1 + dist2 - constant_sum) **2)\n",
        "    indicator = (dist1 + dist2 - constant_sum) != 0\n",
        "    loss_l0 = torch.mean(indicator.float())\n",
        "    print(loss_l0)\n",
        "    loss_l1 = torch.mean(torch.abs(dist1 + dist2 - constant_sum))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # Store current positions for tracking movement\n",
        "    for i in range(num_points):\n",
        "        trajectories[i].append(points[i].detach().cpu().clone().numpy())\n",
        "\n",
        "    # Plot results every 10th epoch\n",
        "    if epoch % 10 == 0:\n",
        "        plot_results(epoch, trajectories, loss_history)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "#Final plot\n",
        "plot_results(epoch, trajectories, loss_history)\n",
        "\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "l2bQcbTG1tEF",
        "outputId": "6ed03454-721c-4ae9-c590-4c2426cb0b6a"
      },
      "id": "l2bQcbTG1tEF",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plot_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8536d4904d52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Plot results every 10th epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_results' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}